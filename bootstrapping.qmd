---
title: "Bootstrapping basics"
date: "June 2023"
author: "Michael Love"
format: 
  beamer:
    fig-width: 5
    fig-height: 3
execute:
  echo: true
  cache: true
---

# sampling variance

```{r echo=FALSE}
library(mvtnorm)
set.seed(1)
n <- 100
mat <- rmvnorm(n,mean=c(0,0),sigma=cbind(c(1,.5),c(.5,1)))
pop <- data.frame(x=mat[,1], y=mat[,2], 
                  color=factor(sample(8,n,TRUE)),
                  shape=sample(16:25,n,TRUE))
little_n <- 20
idx <- sample(n,little_n,FALSE)
dat <- rbind(pop, pop[idx,])
dat$type <- rep(c("population","sample"),c(n,little_n))
library(ggplot2)
ggplot(dat, aes(x,y,color=color,fill=color,shape=shape)) +
  geom_point(size=3, show.legend=FALSE) + 
  scale_shape_identity() +
  facet_wrap(~type)
```

# or when there is not a "population"

```{r echo=FALSE}
grid <- expand.grid(x=-200:200/100,y=-200:200/100)
grid$density <- dmvnorm(grid, mean=c(0,0), 
                        sigma=cbind(c(1,.5),c(.5,1)))
dat$density <- dmvnorm(dat[,1:2], mean=c(0,0), 
                       sigma=cbind(c(1,.5),c(.5,1)))
ggplot(grid, aes(x,y,z=density)) + 
  geom_contour() + 
  geom_point(data=dat[(n+1):(n+little_n),], 
             aes(color=color,fill=color,shape=shape),
             size=3, show.legend=FALSE) + 
  scale_shape_identity()
```

# estimating a parameter

```{r echo=FALSE}
nreps <- 10
samples <- do.call(rbind, replicate(nreps, { 
  pop[sample(n,little_n,FALSE),]
}, simplify = FALSE))
samples$sample <- factor(rep(1:nreps, each=little_n))
fit <- coef(lm(y ~ x, data=dat[1:n,]))
ggplot(samples, aes(x,y,group=sample,color=sample)) +
  geom_jitter(size=3, show.legend=FALSE, alpha=.25, 
              width=.025, height=.025) + 
  stat_smooth(method="lm", se=FALSE) + 
  geom_abline(slope=fit[2], intercept=fit[1], lty=2)
```

# sampling variance $\rightarrow$ parameter estimation

\huge How can we assess the effect of sampling variance 
on parameter estimates?

# idea: sub-sampling a sample

```{r echo=FALSE}
littler_n <- 4
idx <- sample(little_n,littler_n,FALSE)
dat2 <- rbind(dat, dat[(n+1):(n+little_n),][idx,])
dat2$type <- rep(c("population","sample","subsample"),
                 c(n,little_n,littler_n))
ggplot(dat2, aes(x,y,color=color,fill=color,shape=shape)) +
  geom_point(size=3, show.legend=FALSE) + 
  scale_shape_identity() +
  facet_wrap(~type)
```

what is good about the sub-sample? what is a problem?

# idea of bootstrap

* instead of sub-sample, take a sample of the same size
* sample each observation, then put it back ("with replacement")

\vspace{1em}

```{r echo=FALSE, out.width="50%", fig.align="center"}
set.seed(5)
toy <- data.frame(x=1:8,y=rep(2:1,each=8),
                  color=factor(c(1:8,sort(sample(8,8,TRUE)))))
ggplot(toy, aes(x,y,color=color)) +
  geom_point(size=8,show.legend = FALSE) + 
  theme_void() + 
  ylim(0,3)
```

# idea of bootstrap

| Real World          | Bootstrap World           |
|---------------------|---------------------------|
| $P \rightarrow X_n$   | $\widehat{P}_n \rightarrow X^*_n$ |
| $\hat\theta_n = f(X_n)$ | $\hat\theta^*_n = f(X^*_n)$ |

from "Introduction to the Bootstrap"
Efron & Tibshirani (1993)

# bootstrap often used for the variance of an estimator

From Yen-Chi Chen (UW) notes:

* Sample $X^{*(1)}_n, X^{*(2)}_n, \dots, X^{*(B)}_n$
* Obtain 
  $\hat\theta^{*(1)}_n, \hat\theta^{*(2)}_n, \dots, \hat\theta^{*(B)}_n$
* Sample variance of these bootstrap estimates = 
  $\widehat{\textrm{Var}}_B(\hat\theta^*_n)$

Want: 

$$
\widehat{\textrm{Var}}_B(\hat\theta^*_n) \approx \textrm{Var}(\hat\theta_n)
$$

# consistency of the bootstrap variance

From Yen-Chi Chen (UW) notes:

Want: $\widehat{\textrm{Var}}_B(\hat\theta^*_n) \approx \textrm{Var}(\hat\theta_n)$

For large B, we have: 

$$
\widehat{\textrm{Var}}_B(\hat\theta^*_n) \approx 
\textrm{Var}(\hat\theta^*_n|\widehat{P}_n)
$$

Need to show: 

$$
\textrm{Var}(\hat\theta^*_n|\widehat{P}_n) \approx 
\textrm{Var}(\hat\theta_n)
$$

Sketch: for a given estimator, need to show that the variance 
of the functional of the empirical density $\widehat{P}_n$ converges
in probability to the variance of the functional of the original 
density $P$.

# three types of bootstrapping

Consider regression: 

\begin{align}
Y &= X \beta + \varepsilon \\
\varepsilon &\sim N(0, \sigma^2)
\end{align}

* estimate $\widehat{\sigma}^2$
    - simulate new errors $\varepsilon^* \sim N(0,\widehat{\sigma}^2)$
    - simulate new data via $X \hat\beta + \varepsilon^*$
* resample residuals $\hat\varepsilon$ with replacement
    - simulate new data via $X \hat\beta + \hat\varepsilon^*$
* resample cases entirely

what do we assume in these three types?

# example: line with non-normal errors

```{r}
set.seed(1)
n <- 200
x <- runif(n)
eps <- rexp(n, 5)
eps <- eps - mean(eps)
slope <- .2
y <- slope * x + eps
dat <- data.frame(x,y)
```

# example: line with non-normal errors

```{r}
plot(x,y)
abline(0, slope, col="red")
```

# simple bootstrapping

```{r}
library(dplyr)
library(broom)
set.seed(5)
boots <- replicate(1000, {
  idx <- sample(n, replace=TRUE)
  coef(lm(y ~ x, data=dat[idx,]))[2]
})
sd(boots)
fit <- lm(y ~ x, data=dat)
fit %>% tidy() %>% 
  filter(term=="x") %>% pull(std.error)
```

# simple bootstrapping

```{r}
hist(boots, breaks=20)
```

# using `boot`

provides some extra bells and whistles re: stratified data

```{r}
library(boot)
get_slope <- function(data, idx) {
  coef(lm(y ~ x, data=data[idx,]))[2]
}
set.seed(5)
boots2 <- boot(dat, get_slope, R=1000)
```

# using `boot`

```{r}
boots2
```

# using `boot`

```{r}
plot(boots2)
```

# additional options from `car`

```{r}
library(car)
set.seed(5)
boots3 <- Boot(fit, method="residual")
```

# additional options from `car`

```{r}
boots3
```

# additional options from `car`

```{r}
hist(boots3, parm="x")
```

# additional options from `car`

bca = bias-corrected and accelerated, Efron and Tibshirani (1993)

considers:

* proportion of $\hat\theta_n^* < \hat\theta$ 
* skewness of the distribution of $\hat\theta_n^*$

```{r}
confint(boots3)
```

# {infer} package

S, H, G, C =
Specify, Hypothesize, Generate, Calculate

```{r eval=FALSE}
library(infer)
set.seed(5)
perm <- dat %>% specify(y ~ x) %>%
  hypothesize(null="independence") %>%
  generate(reps=1000, type="permute") %>%
  calculate(stat="slope")
```

# bootstrapping a statistic

```{r}
library(infer)
set.seed(5)
boot <- dat %>% specify(y ~ x) %>%
  generate(reps=1000, type="bootstrap") %>%
  calculate(stat="slope")
```

# bootstrapping a statistic

```{r}
visualize(boot, bins=30)
```

# confidence intervals

```{r}
ci <- get_ci(boot)
ci
obs_beta <- dat %>% 
  specify(y ~ x) %>%
  calculate(stat="slope")
```

# visualize

```{r}
visualize(boot, bins=30) +
  shade_confidence_interval(
    ci, alpha=.3, fill="yellow", color="orange") + 
  geom_vline(xintercept=obs_beta$stat, 
             color="red", linewidth=1)
```

# going further

> Bootstrapping Regression Models in R
> An Appendix to An R Companion to Applied Regression, 3rd ed.
> John Fox & Sanford Weisberg

(can find PDF online)
